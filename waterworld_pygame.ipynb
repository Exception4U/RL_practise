{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Waterworld Game in PLE environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ple.games.waterworld import WaterWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "game = WaterWorld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ple import PLE\n",
    "\n",
    "p = PLE(game, fps=30, display_screen=True, force_fps=False)\n",
    "p.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'up': 119, 'right': 100, 'down': 115, 'left': 97}\n"
     ]
    }
   ],
   "source": [
    "print (p.game.actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_frames = 1000\n",
    "reward = 0.0\n",
    "\n",
    "for f in range(nb_frames):\n",
    "    if p.game_over(): #check if the game is over\n",
    "        p.reset_game()\n",
    "\n",
    "    obs = p.getScreenRGB()\n",
    "    action=119\n",
    "    #     action = myAgent.pickAction(reward, obs)\n",
    "    reward = p.act(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "couldn't import doomish\n",
      "Couldn't import doom\n",
      "Score: 0.990 | Reward: 0.990 \n",
      "Score: 0.980 | Reward: -0.010 \n",
      "Score: 0.970 | Reward: -0.010 \n",
      "Score: 0.960 | Reward: -0.010 \n",
      "Score: 0.950 | Reward: -0.010 \n",
      "Score: 0.940 | Reward: -0.010 \n",
      "Score: 0.930 | Reward: -0.010 \n",
      "Score: 0.920 | Reward: -0.010 \n",
      "Score: 0.910 | Reward: -0.010 \n",
      "Score: 0.900 | Reward: -0.010 \n",
      "Score: 0.890 | Reward: -0.010 \n",
      "Score: 0.880 | Reward: -0.010 \n",
      "Score: 0.870 | Reward: -0.010 \n",
      "Score: 0.860 | Reward: -0.010 \n",
      "Score: 0.850 | Reward: -0.010 \n",
      "Score: 0.840 | Reward: -0.010 \n",
      "Score: 0.830 | Reward: -0.010 \n",
      "Score: 0.820 | Reward: -0.010 \n",
      "Score: 0.810 | Reward: -0.010 \n",
      "Score: 0.800 | Reward: -0.010 \n",
      "Score: 0.790 | Reward: -0.010 \n",
      "Score: 0.780 | Reward: -0.010 \n",
      "Score: 0.770 | Reward: -0.010 \n",
      "Score: 0.760 | Reward: -0.010 \n",
      "Score: 0.750 | Reward: -0.010 \n",
      "Score: 0.740 | Reward: -0.010 \n",
      "Score: 0.730 | Reward: -0.010 \n",
      "Score: 0.720 | Reward: -0.010 \n",
      "Score: 0.710 | Reward: -0.010 \n",
      "Score: 0.700 | Reward: -0.010 \n",
      "Score: 0.690 | Reward: -0.010 \n",
      "Score: 0.680 | Reward: -0.010 \n",
      "Score: 0.670 | Reward: -0.010 \n",
      "Score: 0.660 | Reward: -0.010 \n",
      "Score: 0.650 | Reward: -0.010 \n",
      "Score: 0.640 | Reward: -0.010 \n",
      "Score: 0.630 | Reward: -0.010 \n",
      "Score: 0.620 | Reward: -0.010 \n",
      "Score: 0.610 | Reward: -0.010 \n",
      "Score: 0.600 | Reward: -0.010 \n",
      "Score: 0.590 | Reward: -0.010 \n",
      "Score: 0.580 | Reward: -0.010 \n",
      "Score: 0.570 | Reward: -0.010 \n",
      "Score: 0.560 | Reward: -0.010 \n",
      "Score: 0.550 | Reward: -0.010 \n",
      "Score: 0.540 | Reward: -0.010 \n",
      "Score: 0.530 | Reward: -0.010 \n",
      "Score: 0.520 | Reward: -0.010 \n",
      "Score: -0.490 | Reward: -1.010 \n",
      "Score: -0.500 | Reward: -0.010 \n",
      "Score: -0.510 | Reward: -0.010 \n",
      "Score: -0.520 | Reward: -0.010 \n",
      "Score: -0.530 | Reward: -0.010 \n",
      "Score: -0.540 | Reward: -0.010 \n",
      "Score: -0.550 | Reward: -0.010 \n",
      "Score: -0.560 | Reward: -0.010 \n",
      "Score: -0.570 | Reward: -0.010 \n",
      "Score: -0.580 | Reward: -0.010 \n",
      "Score: -0.590 | Reward: -0.010 \n",
      "Score: -0.600 | Reward: -0.010 \n",
      "Score: -0.610 | Reward: -0.010 \n",
      "Score: -0.620 | Reward: -0.010 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-181ec597589d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# random actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Score: {:0.3f} | Reward: {:0.3f} \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/codes/PyGame-Learning-Environment/ple/ple.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \"\"\"\n\u001b[0;32m--> 374\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_oneStepAct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_draw_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/codes/PyGame-Learning-Environment/ple/ple.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \"\"\"\n\u001b[0;32m--> 374\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_oneStepAct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_draw_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/codes/PyGame-Learning-Environment/ple/ple.py\u001b[0m in \u001b[0;36m_oneStepAct\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0mtime_elapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_elapsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/codes/PyGame-Learning-Environment/ple/ple.py\u001b[0m in \u001b[0;36m_draw_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_screen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_oneStepAct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/codes/PyGame-Learning-Environment/ple/games/base/pygamewrapper.py\u001b[0m in \u001b[0;36m_draw_frame\u001b[0;34m(self, draw_screen)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdraw_screen\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetScreenRGB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ple import PLE\n",
    "from ple.games.waterworld import WaterWorld\n",
    "\n",
    "\n",
    "# lets adjust the rewards our agent recieves\n",
    "rewards = {\n",
    "    \"tick\": -0.01,  # each time the game steps forward in time the agent gets -0.1\n",
    "    \"positive\": 1.0,  # each time the agent collects a green circle\n",
    "    \"negative\": -5.0,  # each time the agent bumps into a red circle\n",
    "}\n",
    "\n",
    "# make a PLE instance.\n",
    "# use lower fps so we can see whats happening a little easier\n",
    "game = WaterWorld(width=256, height=256, num_creeps=8)\n",
    "p = PLE(game, fps=15, force_fps=False, display_screen=True,\n",
    "        reward_values=rewards)\n",
    "# we pass in the rewards and PLE will adjust the game for us\n",
    "\n",
    "p.init()\n",
    "actions = p.getActionSet()\n",
    "for i in range(1000):\n",
    "    if p.game_over():\n",
    "        p.reset_game()\n",
    "\n",
    "    action = actions[np.random.randint(0, len(actions))]  # random actions\n",
    "    reward = p.act(action)\n",
    "\n",
    "    print (\"Score: {:0.3f} | Reward: {:0.3f} \".format(p.score(), reward))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN agent class \n",
    "\n",
    "This class is DQN based agent taking image as input\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is enabled with initial size: 90.0% of memory, cuDNN Mixed dnn version. The header is from one version, but we link with a different version (6021, 5110))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# keras and model related\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "import theano.tensor as T\n",
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"theano\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "class ExampleAgent():\n",
    "    \"\"\"\n",
    "        Implements a DQN-ish agent. It has replay memory and epsilon decay. It is missing model freezing. The models are sensitive to the parameters and if applied to other games must be tinkered with.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, batch_size, num_frames,\n",
    "                 frame_skip, lr, discount, rng, optimizer=\"adam\", frame_dim=None):\n",
    "\n",
    "        self.env = env\n",
    "        self.batch_size = batch_size\n",
    "        self.num_frames = num_frames\n",
    "        self.frame_skip = frame_skip\n",
    "        self.lr = lr\n",
    "        self.discount = discount\n",
    "        self.rng = rng\n",
    "\n",
    "        if optimizer == \"adam\":\n",
    "            opt = Adam(lr=self.lr)\n",
    "        elif optimizer == \"sgd\":\n",
    "            opt = SGD(lr=self.lr)\n",
    "        elif optimizer == \"sgd_nesterov\":\n",
    "            opt = SGD(lr=self.lr, nesterov=True)\n",
    "        elif optimizer == \"rmsprop\":\n",
    "            opt = RMSprop(lr=self.lr, rho=0.9, epsilon=0.003)\n",
    "        else:\n",
    "            raise ValueError(\"Unrecognized optmizer\")\n",
    "\n",
    "        self.optimizer = opt\n",
    "\n",
    "        self.frame_dim = self.env.getScreenDims() if frame_dim is None else frame_dim\n",
    "        self.state_shape = (num_frames,) + self.frame_dim\n",
    "        self.input_shape = (batch_size,) + self.state_shape\n",
    "\n",
    "        self.state = deque(maxlen=num_frames)\n",
    "        self.actions = self.env.getActionSet()\n",
    "        self.num_actions = len(self.actions)\n",
    "        self.model = None\n",
    "\n",
    "    def q_loss(self, y_true, y_pred):\n",
    "        # assume clip_delta is 1.0\n",
    "        # along with sum accumulator.\n",
    "        diff = y_true - y_pred\n",
    "        _quad = T.minimum(abs(diff), 1.0)\n",
    "        _lin = abs(diff) - _quad\n",
    "        loss = 0.5 * _quad ** 2 + _lin\n",
    "        loss = T.sum(loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def build_model(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Convolution2D(\n",
    "            16, 8, 8, input_shape=(self.num_frames,) + self.frame_dim,\n",
    "            subsample=(4, 4), activation=\"relu\", init=\"he_uniform\"\n",
    "        ))\n",
    "        model.add(Convolution2D(\n",
    "            16, 4, 4, subsample=(2, 2), activation=\"relu\", init=\"he_uniform\"\n",
    "        ))\n",
    "        model.add(Convolution2D(\n",
    "            32, 3, 3, subsample=(1, 1), activation=\"relu\", init=\"he_uniform\"\n",
    "        ))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(\n",
    "            512, activation=\"relu\", init=\"he_uniform\"\n",
    "        ))\n",
    "        model.add(Dense(\n",
    "            self.num_actions, activation=\"linear\", init=\"he_uniform\"\n",
    "        ))\n",
    "\n",
    "        model.compile(loss=self.q_loss, optimizer=self.optimizer)\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def predict_single(self, state):\n",
    "        \"\"\"\n",
    "            model is expecting a batch_size worth of data. We only have one states worth of\n",
    "            samples so we make an empty batch and set our state as the first row.\n",
    "        \"\"\"\n",
    "        states = np.zeros(self.input_shape)\n",
    "        states[0, ...] = state.reshape(self.state_shape)\n",
    "\n",
    "        return self.model.predict(states)[0]  # only want the first value\n",
    "\n",
    "    def _argmax_rand(self, arr):\n",
    "        # picks a random index if there is a tie\n",
    "        return self.rng.choice(np.where(arr == np.max(arr))[0])\n",
    "\n",
    "    def _best_action(self, state):\n",
    "        q_vals = self.predict_single(state)\n",
    "\n",
    "        return self._argmax_rand(q_vals)  # the action with the best Q-value\n",
    "\n",
    "    def act(self, state, epsilon=1.0):\n",
    "        self.state.append(state)\n",
    "\n",
    "        action = self.rng.randint(0, self.num_actions)\n",
    "        if len(self.state) == self.num_frames:  # we havent seen enough frames\n",
    "            _state = np.array(self.state)\n",
    "\n",
    "            if self.rng.rand() > epsilon:\n",
    "                action = self._best_action(_state)  # exploit\n",
    "\n",
    "        reward = 0.0\n",
    "        for i in range(self.frame_skip):  # we repeat each action a few times\n",
    "            # act on the environment\n",
    "            reward += self.env.act(self.actions[action])\n",
    "\n",
    "        reward = np.clip(reward, -1.0, 1.0)\n",
    "\n",
    "        return reward, action\n",
    "\n",
    "    def start_episode(self, N=3):\n",
    "        self.env.reset_game()  # reset\n",
    "        for i in range(self.rng.randint(N)):\n",
    "            self.env.act(self.env.NOOP)  # perform a NOOP\n",
    "\n",
    "    def end_episode(self):\n",
    "        self.state.clear()\n",
    "\n",
    "\n",
    "class ReplayMemory():\n",
    "\n",
    "    def __init__(self, max_size, min_size):\n",
    "        self.min_replay_size = min_size\n",
    "        self.memory = deque(maxlen=max_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def add(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def train_agent_batch(self, agent):\n",
    "        if len(self.memory) > self.min_replay_size:\n",
    "            states, targets = self._random_batch(agent)  # get a random batch\n",
    "            return agent.model.train_on_batch(states, targets)  # ERR?\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _random_batch(self, agent):\n",
    "        inputs = np.zeros(agent.input_shape)\n",
    "        targets = np.zeros((agent.batch_size, agent.num_actions))\n",
    "\n",
    "        seen = []\n",
    "        idx = agent.rng.randint(\n",
    "            0,\n",
    "            high=len(\n",
    "                self.memory) -\n",
    "            agent.num_frames -\n",
    "            1)\n",
    "\n",
    "        for i in range(agent.batch_size):\n",
    "            while idx in seen:\n",
    "                idx = agent.rng.randint(0, high=len(\n",
    "                    self.memory) - agent.num_frames - 1)\n",
    "\n",
    "            states = np.array([self.memory[idx + j][0]\n",
    "                               for j in range(agent.num_frames + 1)])\n",
    "            art = np.array([self.memory[idx + j][1:]\n",
    "                            for j in range(agent.num_frames)])\n",
    "\n",
    "            actions = art[:, 0].astype(int)\n",
    "            rewards = art[:, 1]\n",
    "            terminals = art[:, 2]\n",
    "\n",
    "            state = states[:-1]\n",
    "            state_next = states[1:]\n",
    "\n",
    "            inputs[i, ...] = state.reshape(agent.state_shape)\n",
    "            # we could make zeros but pointless.\n",
    "            targets[i] = agent.predict_single(state)\n",
    "            Q_prime = np.max(agent.predict_single(state_next))\n",
    "\n",
    "            targets[i, actions] = rewards + \\\n",
    "                (1 - terminals) * (agent.discount * Q_prime)\n",
    "\n",
    "            seen.append(idx)\n",
    "\n",
    "        return inputs, targets\n",
    "\n",
    "\n",
    "def loop_play_forever(env, agent):\n",
    "    # our forever play loop\n",
    "    try:\n",
    "        # slow it down\n",
    "        env.display_screen = True\n",
    "        env.force_fps = False\n",
    "\n",
    "        while True:\n",
    "            agent.start_episode()\n",
    "            episode_reward = 0.0\n",
    "            while env.game_over() == False:\n",
    "                state = env.getGameState()\n",
    "                reward, action = agent.act(state, epsilon=0.05)\n",
    "                episode_reward += reward\n",
    "\n",
    "            print (\"Agent score {:0.1f} reward for episode.\".format(episode_reward))\n",
    "            agent.end_episode()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print (\"Exiting out!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (6,) (4,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-11b4a397eaf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# for our agent.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWaterWorld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPLE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_preprocessor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnv_state_preprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_screen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_fps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     agent = Agent(env, batch_size, num_frames, frame_skip, lr,\n",
      "\u001b[0;32m~/codes/PyGame-Learning-Environment/ple/ple.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, game, fps, frame_skip, num_steps, reward_values, force_fps, display_screen, add_noop_action, state_preprocessor, rng)\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \"Asked to return non-visual state on game that does not support it!\")\n\u001b[1;32m    143\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_preprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallowed_fps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfps\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallowed_fps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-11b4a397eaf5>\u001b[0m in \u001b[0;36mnv_state_preprocessor\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mmax_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m128.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m#     import pdb;pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (6,) (4,) "
     ]
    }
   ],
   "source": [
    "# thanks to @edersantana and @fchollet for suggestions & help.\n",
    "\n",
    "import numpy as np\n",
    "from ple import PLE  # our environment\n",
    "from ple.games.catcher import Catcher\n",
    "from ple.games.waterworld import WaterWorld\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# from example_support import ExampleAgent, ReplayMemory, loop_play_forever\n",
    "\n",
    "\n",
    "class Agent(ExampleAgent):\n",
    "    \"\"\"\n",
    "        Our agent takes 1D inputs which are flattened.\n",
    "        We define a full connected model below.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        ExampleAgent.__init__(self, *args, **kwargs)\n",
    "\n",
    "        self.state_dim = self.env.getGameStateDims()\n",
    "        self.state_shape = np.prod((num_frames,) + self.state_dim)\n",
    "        self.input_shape = (batch_size, self.state_shape)\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(\n",
    "            input_dim=self.state_shape, output_dim=256, activation=\"relu\", init=\"he_uniform\"\n",
    "        ))\n",
    "        model.add(Dense(\n",
    "            512, activation=\"relu\", init=\"he_uniform\"\n",
    "        ))\n",
    "        model.add(Dense(\n",
    "            self.num_actions, activation=\"linear\", init=\"he_uniform\"\n",
    "        ))\n",
    "\n",
    "        model.compile(loss=self.q_loss, optimizer=SGD(lr=self.lr))\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "\n",
    "def nv_state_preprocessor(state):\n",
    "    \"\"\"\n",
    "        This preprocesses our state from PLE. We rescale the values to be between\n",
    "        0,1 and -1,1.\n",
    "    \"\"\"\n",
    "    # taken by inspection of source code. Better way is on its way!\n",
    "    max_values = np.array([128.0, 20.0, 128.0, 128.0])\n",
    "#     import pdb;pdb.set_trace()\n",
    "    state = np.array(list(state.values())) / max_values\n",
    "\n",
    "    return state.flatten()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # this takes about 15 epochs to converge to something that performs decently.\n",
    "    # feel free to play with the parameters below.\n",
    "\n",
    "    # training parameters\n",
    "    num_epochs = 15\n",
    "    num_steps_train = 15000  # steps per epoch of training\n",
    "    num_steps_test = 3000\n",
    "    update_frequency = 4  # step frequency of model training/updates\n",
    "\n",
    "    # agent settings\n",
    "    batch_size = 32\n",
    "    num_frames = 4  # number of frames in a 'state'\n",
    "    frame_skip = 2\n",
    "    # percentage of time we perform a random action, help exploration.\n",
    "    epsilon = 0.15\n",
    "    epsilon_steps = 30000  # decay steps\n",
    "    epsilon_min = 0.1\n",
    "    lr = 0.01\n",
    "    discount = 0.95  # discount factor\n",
    "    rng = np.random.RandomState(24)\n",
    "\n",
    "    # memory settings\n",
    "    max_memory_size = 100000\n",
    "    min_memory_size = 1000  # number needed before model training starts\n",
    "\n",
    "    epsilon_rate = (epsilon - epsilon_min) / epsilon_steps\n",
    "\n",
    "    # PLE takes our game and the state_preprocessor. It will process the state\n",
    "    # for our agent.\n",
    "    game = WaterWorld(width=128, height=128)\n",
    "    env = PLE(game, fps=60, state_preprocessor=nv_state_preprocessor, display_screen=True, force_fps=False)\n",
    "\n",
    "    agent = Agent(env, batch_size, num_frames, frame_skip, lr,\n",
    "                  discount, rng, optimizer=\"sgd_nesterov\")\n",
    "    agent.build_model()\n",
    "\n",
    "    memory = ReplayMemory(max_memory_size, min_memory_size)\n",
    "\n",
    "    env.init()\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        steps, num_episodes = 0, 0\n",
    "        losses, rewards = [], []\n",
    "        env.display_screen = False\n",
    "\n",
    "        # training loop\n",
    "        while steps < num_steps_train:\n",
    "            episode_reward = 0.0\n",
    "            agent.start_episode()\n",
    "\n",
    "            while env.game_over() == False and steps < num_steps_train:\n",
    "                state = env.getGameState()\n",
    "                reward, action = agent.act(state, epsilon=epsilon)\n",
    "                memory.add([state, action, reward, env.game_over()])\n",
    "\n",
    "                if steps % update_frequency == 0:\n",
    "                    loss = memory.train_agent_batch(agent)\n",
    "\n",
    "                    if loss is not None:\n",
    "                        losses.append(loss)\n",
    "                        epsilon = np.max([epsilon_min, epsilon - epsilon_rate])\n",
    "\n",
    "                episode_reward += reward\n",
    "                steps += 1\n",
    "\n",
    "            if num_episodes % 5 == 0:\n",
    "                print (\"Episode {:01d}: Reward {:0.1f}\".format(num_episodes, episode_reward))\n",
    "\n",
    "            rewards.append(episode_reward)\n",
    "            num_episodes += 1\n",
    "            agent.end_episode()\n",
    "\n",
    "        print (\"\\nTrain Epoch {:02d}: Epsilon {:0.4f} | Avg. Loss {:0.3f} | Avg. Reward {:0.3f}\".format(epoch, epsilon, np.mean(losses), np.sum(rewards) / num_episodes)\n",
    ")\n",
    "        steps, num_episodes = 0, 0\n",
    "        losses, rewards = [], []\n",
    "\n",
    "        # display the screen\n",
    "        env.display_screen = True\n",
    "\n",
    "        # slow it down so we can watch it fail!\n",
    "        env.force_fps = False\n",
    "\n",
    "        # testing loop\n",
    "        while steps < num_steps_test:\n",
    "            episode_reward = 0.0\n",
    "            agent.start_episode()\n",
    "\n",
    "            while env.game_over() == False and steps < num_steps_test:\n",
    "                state = env.getGameState()\n",
    "                reward, action = agent.act(state, epsilon=0.05)\n",
    "\n",
    "                episode_reward += reward\n",
    "                steps += 1\n",
    "\n",
    "                # done watching after 500 steps.\n",
    "                if steps > 500:\n",
    "                    env.force_fps = True\n",
    "                    env.display_screen = False\n",
    "\n",
    "            if num_episodes % 5 == 0:\n",
    "                print (\"Episode {:01d}: Reward {:0.1f}\".format(num_episodes, episode_reward))\n",
    "\n",
    "            rewards.append(episode_reward)\n",
    "            num_episodes += 1\n",
    "            agent.end_episode()\n",
    "\n",
    "        print (\"Test Epoch {:02d}: Best Reward {:0.3f} | Avg. Reward {:0.3f}\".format(epoch, np.max(rewards), np.sum(rewards) / num_episodes))\n",
    "\n",
    "    print (\"\\nTraining complete. Will loop forever playing!\")\n",
    "    loop_play_forever(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([0.12, -.3333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_py3",
   "language": "python",
   "name": "tensorflow_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
